---
title: "Classification and Prediction"
author: "Imran Ali"
date: "March 31, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Are we already using a classifier?

![gmail classifying spam](spam.jpg)

## What is spam?

**Spam:** _irrelevant messages_ sent over the internet,  typically to a _large number of users_
for the _purpose of_   

- advertising
- phishing 
- spreading malware

**phishing:** The fraudulent practice of sending emails purporting to be from reputable companies in order to induce individuals to reveal personal information, such as passwords and credit card numbers.

**malware:** software which is specifically designed to disrupt, damage, or gain authorized access to a computer system.

## How can spam be identified?
Inspecting various attributes of an email e.g.

1. Number of recipient 
2. Unknown sender
3. Keywords or phrases in subject or body of the email
4. size of the content

## Some questions to ponder

1. Are all attributes equally important?
2. How to decide which attribute is more important?

## Data Classification

![Learning from data](classification1.png)

![Classification](classification2.png)


## Classification using Decision Trees
### What is a decision tree?

It is a (flowchart like) **_tree structure_**

![A Decision Tree for the concept _buys_computer_](decisionTree.png)

- Decision Tree indicating whether a customer is likely to purchase a computer
- Each **internal node** represents a **test** on an attribute  
- Each **leaf node** represents a **class** (either yes or no)  




## Induction of Decision Tree

### Sample data
```{r, echo=FALSE}
data <- read.csv(file = "data.csv", header = TRUE)
knitr::kable(data)
```

### Algorithm: Generate decision tree

**Input:**

- Data partition, D, which is a set of training tuples and their associated class labels;  
- attribute list, the set of candidate attributes;  
- Attribute selection method, a procedure to determine the splitting criterion that “best” par-
titions the data tuples into individual classes. This criterion consists of a splitting attribute
and, possibly, either a split point or splitting subset.  

**Output:**
A decision tree.

**Method:**

```{r, eval=FALSE}
1. create a node N;
2. if tuples in D are all of the same class, C then
3. return N as a leaf node labeled with the class C;
4. if attribute list is empty then
5. return N as a leaf node labeled with the majority class in D; // majority voting
6. apply AttributeSelectionMethod(D, attribute list) to find the “best” splitting criterion;
7. label node N with splitting criterion;
8. if splitting attribute is discrete-valued and
 multiway splits allowed then // not restricted to binary trees
9. attribute list ← attribute list − splitting attribute; // remove splitting attribute
10. for each outcome j of splitting criterion
// partition the tuples and grow subtrees for each partition
11. let Dj be the set of data tuples in D satisfying outcome j; // a partition
12. if Dj is empty then
13. attach a leaf labeled with the majority class in D to node N;
14. else attach the node returned by GenerateDecisionTree(Dj , attribute list) to node N;
endfor
15. return N;
```

## Useful Online resources

[A blog post by Luis Serrano](https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4) explaining entropy, information gain 

[A question on cross validated](https://stats.stackexchange.com/questions/329756/what-is-the-significance-of-the-log-base-being-2-in-entropy) discussing log base